---
title: "Introduction to abc4d"
author: "Valeria"
date: "4/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## abc4d

The goal of abc4d is to aim researchers through the steps of data cleaning, pre-processing and analysis of whole-brain microscopy data (over time). To get started, you can install abc4d from github. 

```{r install}
# install devtools if not yet available
#install.packages("devtools")

#devtools::install_github("valeriabonapersona/abc4d")
```

A copy of the abc4d cheatsheet can be found here (LINK). Of note, the data resources provided within the app are based on the Allen Brain Reference Atlas, and the use of the Clearmap software for cell annotation. These can be substituted by using the atlas of interest in the resource preparation step, see vignettes and tutorials for more details.

## From annotated cells to data analysis
After imaging your brains, you have probably used a software to transform images into machine-readable numbers. Examples of open softwares are ClearMap, brainrender and whole-brain. These softwares perform cell detection, alignment of the brain to an Atlas, and annotation of the detected cells. To prepare the data for analysis, the data needs to be cleaned (Step 1) and pre-processed (Step 2). 

### Step 1: data cleaning
During data cleaning, artefacts are removed, brain areas are selected, damaged brain areas are removed and re-imputed. This step is necessary no matter what the analysis will be.

First, resources for data cleaning need to be prepared. These are: 

- A list of the damaged areas
- The tree structure of the categorized areas (see Tutorial (LINK))
- The masks to correct for artifacts (provided as data("halo_masks") and data("vent_mask"))

For more information, see the vignette of clean_counts().

clean_counts() runs for each sample separately.

```{r data_cleaning, eval=FALSE, include=FALSE}
# # resources to prepare, see vignette for correct structure
# data # load the xyz coordinates
# atlas # resource with the categorization of the brain areas of interest
# damaged_areas # list of damaged areas
# out_mask # mask to correct outer artefacts
# vent_mask # mask to correct ventricles' artefacts
# 
# # cleaning the data (runs one sample)
# clean_counts(sample_id, data, atlas, damaged_areas,
#              out_mask = out_mask, vent_mask = vent_mask,
#              path_cleaned = "path_to_cleaned_data",
#              path_removed = "path_to_summary_removed_data")
```

With clean_counts(), the cleaned xyz coordinates will be saved at the specified folder. clean_counts() is run for each sample independently. Next, we merge all samples in one dataframe.

```{r}
# counts_cleaned <- "path/to/cleaned/data/"
# all_cells <- samples_files_to_df(counts_cleaned)
```

Now you have cleaned xyz coordinates in just one dataframe.

### Step 2: preprocessing
Pre-processing depends on which (region-based) analysis will be conducted. Here, we  normalize to the size of the brain area, we transform and standardize the data to control for batch effects. 

The functions: 

- summarize_per_region() transforms xyz coordinates into a summary per region
- normalize() normalizes the brain regions based on their size using the Ero et al. atlas
- preprocess_per_region() performs box-cox transformation and z-score normalization to correct for batch effects.

This figure summarizes which preprocessing step is required for which analysis.

Your data is now ready to be analyzed. 

## Analysis
Analysis can be performed at different levels of spatial resolution, from single-cell to networks. Here we provide functions for different analyses. Please see the cheatsheet (LINK) for a summary. 
